{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-in-Arabic_Lab7-TermRepresentations&Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telsayed/IR-in-Arabic/blob/master/Summer2021/labs/day7/IR_in_Arabic_Lab7_TermRepresentations%26Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIOx_DMOhRPW"
      },
      "source": [
        "# **IR in Arabic** - Summer 2021 lab day7\n",
        "\n",
        "This is one of a series of Colab notebooks created for the **IR in Arabic** course. It demonstrates how we can get word embeddings using a non context-aware and a context-aware model and how they differ.\n",
        "\n",
        "The **learning outcomes** of the this notebook are:\n",
        "\n",
        "\n",
        "*   Get words and documents embeddings using a non context-aware and a context-aware model.\n",
        "*   Compare the similarity of words and documents embeddings of both models.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CRVdfNCi_B7"
      },
      "source": [
        "### **Setup**\n",
        "\n",
        "We will use [FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP](https://www.aclweb.org/anthology/N19-4010/). FLAIR make it easy to get words and documents embeddings using a huge number of SOTA models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uxlak0PvNVU"
      },
      "source": [
        "#install FLAIR\n",
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CujHitStvRaR"
      },
      "source": [
        "#we need to install allennlp in order to be able to use elmo model\n",
        "!pip install allennlp==0.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohr0b0hZMs0L"
      },
      "source": [
        "### **Word embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMx78RIJqGPI"
      },
      "source": [
        "We will first use a non context-aware model **GloVe** to get word embeddings as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAoO4bAhvXAs"
      },
      "source": [
        "from termcolor import colored\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import WordEmbeddings\n",
        "\n",
        "# initialize embedding by specifying which model we want to use\n",
        "glove_embedding = WordEmbeddings('glove')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HaPZTi1vYVD"
      },
      "source": [
        "# create sentence. Sentence class holds all meta related to a text\n",
        "glove_sentence = Sentence('We are travelling to Italy to watch a famous play')\n",
        "print(glove_sentence)\n",
        "print(glove_sentence.tokens)\n",
        "#Sentence will split our sentence to tokens. Let's access the first token\n",
        "print(glove_sentence[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoHKuN3Z4jz0"
      },
      "source": [
        "#print each token embedding. We will get empty vectors because we did not get the embeddings yet\n",
        "for token in glove_sentence:\n",
        "    print(colored(token,attrs=['bold']))\n",
        "    #print the embedding for each token\n",
        "    print(token.embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC20I21xyOM0"
      },
      "source": [
        "# embed a sentence using glove.\n",
        "glove_embedding.embed(glove_sentence)\n",
        "# now check out the embedded tokens.\n",
        "for token in glove_sentence:\n",
        "    print(colored(token,attrs=['bold']))\n",
        "    #print the embedding for each token\n",
        "    print(token.embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2lTTOZ8y-PL"
      },
      "source": [
        "#print the embedding for the word \"play\"\n",
        "print(colored(\"The embedding of the word play\",attrs=['bold']))\n",
        "print(glove_sentence[9].embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQDHTNOB7Zir"
      },
      "source": [
        "#print the length of the embedding vector\n",
        "print(colored(\"The size of the embedding vector of the word play\",attrs=['bold']))\n",
        "len(glove_sentence[9].embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jad27v-Pqvvq"
      },
      "source": [
        "Let's create another sentence that contains the word **\"play\"** but with a different meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXbFZZLgyxUz"
      },
      "source": [
        "# create sentence.\n",
        "glove_sentence2 = Sentence('They play tennis on their break')\n",
        "\n",
        "# embed a sentence using glove.\n",
        "glove_embedding.embed(glove_sentence2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZB-zK48zMiu"
      },
      "source": [
        "#print the embedding of the word \"play\" in the first sentence\n",
        "print(colored(\"The embedding of the word play in the first sentence\",attrs=['bold']))\n",
        "print(glove_sentence[9].embedding)\n",
        "#print the embedding for the word \"play\" you will notice it is similar to the emebdding of \"play\" in the previous sentence\n",
        "print(colored(\"The embedding of the word play in the second sentence\",attrs=['bold']))\n",
        "print(glove_sentence2[1].embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWxCSU3M179t"
      },
      "source": [
        "Check if the word **\"play\"** have the same embeddings in both sentences when **GloVe** was used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrQvADTU1fxh"
      },
      "source": [
        "from scipy import spatial\n",
        "similarity= 1 - spatial.distance.cosine(glove_sentence[9].embedding, glove_sentence2[1].embedding)\n",
        "similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E50K33XqrfG0"
      },
      "source": [
        "Let's try the context-aware model **ELMo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3giN2KMtvdN7"
      },
      "source": [
        "from flair.embeddings import ELMoEmbeddings\n",
        "# initialize embedding\n",
        "embedding = ELMoEmbeddings()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOFhSQ7T5tpQ"
      },
      "source": [
        "ELMo word embeddings can be constructed by combining ELMo layers in different ways. The available combination strategies are:\n",
        "\n",
        "*  **\"all\"**: Use the concatenation of the three ELMo layers.\n",
        "*  **\"top\":** Use the top ELMo layer.\n",
        "* **\"average\":** Use the average of the three ELMo layers.\n",
        "\n",
        "By default, the top 3 layers are concatenated to form the word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUP7Ail1wf5H",
        "outputId": "37c259e9-e8c8-432a-9447-6d9b8d001151"
      },
      "source": [
        "# create a sentence\n",
        "elmo_sentence = Sentence('We are travelling to Italy to watch a famous play')\n",
        "\n",
        "# embed words in sentence\n",
        "embedding.embed(elmo_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"We are travelling to Italy to watch a famous play\"   [âˆ’ Tokens: 10]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS0zVrntxLJy"
      },
      "source": [
        "# now check out the embedded tokens.\n",
        "for token in elmo_sentence:\n",
        "    print(colored(token,attrs=['bold']))\n",
        "    print(token.embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWVzoBYazkLO"
      },
      "source": [
        "#print the embedding for the word \"play\"\n",
        "print(colored(\"The embedding of the word play\",attrs=['bold']))\n",
        "elmo_sentence[9].embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng1fWjz26-lj"
      },
      "source": [
        "#print the length of the embedding vector\n",
        "print(colored(\"The size of the embedding vector of the word play\",attrs=['bold']))\n",
        "#the length will be 3072 as it is the concatention of the top 3 layers each with a length of 1,024 \n",
        "len(elmo_sentence[9].embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaR0rO5N0jld"
      },
      "source": [
        "Let's get the elmo embedding of the second sentence we used previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7B5KL9izU0W",
        "outputId": "e5144f43-7bf3-406b-f6ac-bbe8ca63d86f"
      },
      "source": [
        "# create a sentence\n",
        "elmo_sentence2 = Sentence('They play tennis on their break')\n",
        "# embed words in sentence\n",
        "embedding.embed(elmo_sentence2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"They play tennis on their break\"   [âˆ’ Tokens: 6]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBpq8XNDz2hq"
      },
      "source": [
        "#print the embedding of the word \"play\" in the first sentence\n",
        "print(colored(\"The embedding of the word play in the first sentence\",attrs=['bold']))\n",
        "print(elmo_sentence[9].embedding)\n",
        "#print the embedding for the word \"play\" you will notice it is similar to the emebdding of \"play\" in the previous sentence\n",
        "print(colored(\"The embedding of the word play in the second sentence\",attrs=['bold']))\n",
        "print(elmo_sentence2[1].embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpOMP47N3O7R"
      },
      "source": [
        "Check if the word **\"play\"** have the same embeddings in both sentences when **ELMo** was used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST5ymrUa1RCy"
      },
      "source": [
        "similarity = 1 - spatial.distance.cosine(elmo_sentence[9].embedding, elmo_sentence2[1].embedding)\n",
        "similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uozu0MFjUoB4"
      },
      "source": [
        "**Notice that the similarity between the words is equal 1 when GloVe was used which means they are exacly similar while it is low when ELMo was used because it is a contextual model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9-9mwLRMJ2O"
      },
      "source": [
        "### **Document embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABGpIPyK7f1P"
      },
      "source": [
        "If we want to get the document embedding we perform pooling (the average of words embeddings in our case) over all the tokens embeddings as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuC_QcmgsyTQ"
      },
      "source": [
        "**Documents embeddings using GloVe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqkdaPTz7yg7"
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
        "\n",
        "# initialize the word embeddings\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "# initialize the document embeddings, mode = mean\n",
        "document_embeddings = DocumentPoolEmbeddings([glove_embedding])\n",
        "\n",
        "#Now, create two sentences and call the embedding's embed() method .\n",
        "glove_sentence = Sentence('We are travelling to Italy to watch a famous play')\n",
        "glove_sentence2 = Sentence('They play tennis on their break')\n",
        "# embed the sentences with our document embedding\n",
        "document_embeddings.embed(glove_sentence)\n",
        "document_embeddings.embed(glove_sentence2)\n",
        "# now check out the embedded sentences.\n",
        "print(colored(\"The embedding of first sentence\",attrs=['bold']))\n",
        "print(glove_sentence.embedding)\n",
        "print(colored(\"The embedding of second sentence\",attrs=['bold']))\n",
        "print(glove_sentence2.embedding)\n",
        "\n",
        "#check the size of the vector. Since it is the average of the terms vectors it will be 100\n",
        "print(colored(\"The size of the embedding vector of second sentence\",attrs=['bold']))\n",
        "len(glove_sentence2.embedding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4NQwH0QQ5eF"
      },
      "source": [
        "\n",
        "Let's check the similarity between these sentences embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNKeI8tNQ9gd"
      },
      "source": [
        "similarity = 1 - spatial.distance.cosine(glove_sentence.embedding, glove_sentence2.embedding)\n",
        "similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAiGOv6As633"
      },
      "source": [
        "**Documents embeddings using ELMo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npi2R61J8MW0"
      },
      "source": [
        "from flair.embeddings import ELMoEmbeddings, DocumentPoolEmbeddings\n",
        "# init embedding\n",
        "elmo_embedding = ELMoEmbeddings()\n",
        "\n",
        "# initialize the document embeddings, mode = mean\n",
        "document_embeddings = DocumentPoolEmbeddings([elmo_embedding])\n",
        "\n",
        "#Now, create two sentences and call the embedding's embed() method .\n",
        "elmo_sentence = Sentence('We are travelling to Italy to watch a famous play')\n",
        "elmo_sentence2 = Sentence('They play tennis on their break')\n",
        "# embed the sentences with our document embedding\n",
        "document_embeddings.embed(elmo_sentence)\n",
        "document_embeddings.embed(elmo_sentence2)\n",
        "# now check out the embedded sentences.\n",
        "print(colored(\"The embedding of first sentence\",attrs=['bold']))\n",
        "print(elmo_sentence.embedding)\n",
        "print(colored(\"The embedding of second sentence\",attrs=['bold']))\n",
        "print(elmo_sentence2.embedding)\n",
        "print(colored(\"The size of the embedding vector of second sentence\",attrs=['bold']))\n",
        "len(elmo_sentence2.embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "228uBgTwRnPk"
      },
      "source": [
        "similarity = 1 - spatial.distance.cosine(elmo_sentence.embedding, elmo_sentence2.embedding)\n",
        "similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qf9ylBIs-mi"
      },
      "source": [
        "## **Combining non-contextual and contextual embeddings**\n",
        "\n",
        "In some cases we may need to combine both embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao5beho_Bwez"
      },
      "source": [
        "You can combine both non-contextual and contextual embeddings easily as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7R8bNYOCBmV"
      },
      "source": [
        "from flair.embeddings import StackedEmbeddings\n",
        "\n",
        "stacked_embeddings = StackedEmbeddings([\n",
        "                                        glove_embedding,\n",
        "                                        elmo_embedding\n",
        "                                       ])\n",
        "sentence = Sentence('They play tennis on their break')\n",
        "\n",
        "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
        "stacked_embeddings.embed(sentence)\n",
        "\n",
        "# now check out the embedded tokens.\n",
        "for token in sentence:\n",
        "    print(colored(token,attrs=['bold']))\n",
        "    print(token.embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mduaEyjNC2Vg"
      },
      "source": [
        "Words are now embedded using a concatenation of two different embeddings. This means that the resulting embedding vector is still a single PyTorch vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rwp7KJrCZA3"
      },
      "source": [
        "print(colored(\"The size of the embedding vector for the word play\",attrs=['bold']))\n",
        "print(len(sentence[1].embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eztgjHERC93p"
      },
      "source": [
        "print(colored(\"The embedding vector for the word play\",attrs=['bold']))\n",
        "print(sentence[1].embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FU1kIJLDXHa"
      },
      "source": [
        "#check the glove embeddings which are the first 100 elements\n",
        "print(sentence[1].embedding[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kZD5pZ5DhmZ"
      },
      "source": [
        "#check the elmo embeddings which are the vector elements from 100 until the end of the vector\n",
        "print(sentence[1].embedding[100:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmvFsKKN9WeX"
      },
      "source": [
        "### **Exercise1**\n",
        "Choose either the word \"rose\" or \"tie\" to create two different sentences such that they share the same word but with different meanings. Use both ELMo and GloVe to get the words embeddings. Check the similarity between the embeddings of the common word in both sentences when GloVe and ELMo were used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01fOLbFa9muy"
      },
      "source": [
        "#add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cko0MjxkR_xQ"
      },
      "source": [
        "### **Exercise2**\n",
        "\n",
        "Get the document embeddings for the sentences that you created using both ELMo and GloVe. Compute the similarity between the GloVe sentences embeddings and compare it to the simliarity between the ELMo sentences embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gic-7x39qSv"
      },
      "source": [
        "#add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QBkFAYF50BY"
      },
      "source": [
        "### **References**\n",
        "\n",
        "\n",
        "*   [Flair word emebddings tutorial.](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md)\n",
        "*   [Flair Elmo embedding tutorial.](https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/ELMO_EMBEDDINGS.**md**)\n",
        "* [Flair document embeddings tutorial.](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md)\n",
        "\n"
      ]
    }
  ]
}