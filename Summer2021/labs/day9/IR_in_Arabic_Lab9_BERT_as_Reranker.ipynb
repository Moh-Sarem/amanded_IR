{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IR-in-Arabic_Lab9_BERT_as_Reranker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telsayed/IR-in-Arabic/blob/master/Summer2021/labs/day9/IR_in_Arabic_Lab9_BERT_as_Reranker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzUxXUVMVVU"
      },
      "source": [
        "\n",
        "\n",
        "# **IR in Arabic** - Summer 2021 lab notebook , **Day 9**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjTczIiMctt"
      },
      "source": [
        "This is one of a series of Colab notebooks created for the **IR in Arabic** course. This lab aims to fine-tune BERT transformer model and use it for re-ranking.\n",
        "\n",
        "The **learning outcomes** of the this notebook are:\n",
        "1. Do fine-tuning BERT for a specific task.\n",
        "2. Perform re-ranking using BERT.\n",
        "3. Save and load pre-trained BERT models.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vSBU5h9U_C"
      },
      "source": [
        "## **Utilize the GPU of Colab**\n",
        "In this session, we will work on experiments that require GPU to run. To make the experiments running over the GPU provided by Colab, you need to do the following:\n",
        "\n",
        "1. Go to Menu > Runtime > Change runtime.\n",
        "\n",
        "2. Change hardware acceleration to GPU.\n",
        "\n",
        "Then run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0I7C40zV12G"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "# else:\n",
        "#     raise SystemError('GPU device not found')\n",
        "\n",
        "# Choose GPU as device to run the experiments on \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkS6LLkX6HHV"
      },
      "source": [
        "## **Hugging Face** \n",
        "[Hugging face](https://huggingface.co/) is an NLP-focused startup with a large open-source community, in particular around the Transformers library. ðŸ¤— Transformers is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT, that obtain state-of-the-art results on a variety of NLP tasks like text classification, information extraction, question answering, and text generation. Those architectures come pre-trained with several sets of weights. Getting started with Transformers only requires to install the pip package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sKgPMd_-gU"
      },
      "source": [
        "#install the transformer library\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmC0oRLO_2c6"
      },
      "source": [
        "# install needed libraries\n",
        "#install the Pyterrier framework\n",
        "!pip install python-terrier\n",
        "#install the Arabic stop words library\n",
        "!pip install Arabic-Stopwords\n",
        "#we need to import the following libraries.\n",
        "import pandas as pd\n",
        "#to display the full text on the notebook without truncation\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "import re\n",
        "from snowballstemmer import stemmer\n",
        "from tqdm import tqdm\n",
        "import arabicstopwords.arabicstopwords as stp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh1LhtY8lpbt"
      },
      "source": [
        "# Use our indexed EveTAR dataset. The index is uploaded in our Github repository \n",
        "%rm -rf IR-in-Arabic\n",
        "%rm -rf evetarIndex\n",
        "!git clone https://github.com/telsayed/IR-in-Arabic.git \n",
        "!unzip IR-in-Arabic/Summer2021/data/EveTAR/evetarIndex.zip -d evetarIndex\n",
        "!ls evetarIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEZ2cOG5BVhZ"
      },
      "source": [
        "# import needed libraries\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.utils import class_weight\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import gc\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHKmI5M3ipTT"
      },
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce5_5TNTSgCa"
      },
      "source": [
        "Here we provide some helper functions that will help in preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qByMgTb4SfBL"
      },
      "source": [
        "\n",
        "def clean(text):\n",
        "\n",
        "  '''\n",
        "  Clean input text form urls, handles, tabs, line jumps, and extra white spaces\n",
        "  '''\n",
        "  text = re.sub(r\"http\\S+\", \" \", text)  # remove urls\n",
        "  text = re.sub(r\"RT \", \" \", text)  # remove rt\n",
        "  text = re.sub(r\"@[\\w]*\", \" \", text)  # remove handles\n",
        "  # text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text)# remove special characters\n",
        "  text = re.sub(r\"\\t\", \" \", text)  # remove tabs\n",
        "  text = re.sub(r\"\\n\", \" \", text)  # remove line jump\n",
        "  text = re.sub(r\"\\s+\", \" \", text)  # remove extra white space\n",
        "  text = text.strip()\n",
        "  return text\n",
        "\n",
        "#removing stop sords function\n",
        "def remove_stop_words(sentence):\n",
        "  terms=[]\n",
        "  stopWords= set(stp.stopwords_list())\n",
        "  for term in sentence.split() : \n",
        "      if term not in stopWords :\n",
        "          terms.append(term)\n",
        "  return \" \".join(terms)\n",
        "\n",
        "#a function to normalize the tweets\n",
        "def normalize(text):\n",
        "  text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
        "  text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
        "  text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
        "  text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
        "  text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
        "  return(text)\n",
        "\n",
        "#define the stemming function\n",
        "ar_stemmer = stemmer(\"arabic\")\n",
        "def stem(sentence):\n",
        "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])\n",
        "\n",
        "\n",
        "def preprocess(sentence):\n",
        "  # apply preprocessing steps on the given sentence\n",
        "  sentence =remove_stop_words(sentence)\n",
        "  sentence =normalize(sentence)\n",
        "  sentence =stem(sentence)\n",
        "  return sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwv_0o1Gnyb9"
      },
      "source": [
        "# we will load the index we created before\n",
        "index_ref = pt.IndexRef.of(\"./evetarIndex/data.properties\")\n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x63JjSBKkZuP"
      },
      "source": [
        "# load the collection\n",
        "dataset_links=[\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-01.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-02.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-03.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-04.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-05.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-06.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-07.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-08.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-09.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-10.txt\"]\n",
        "\n",
        "full_data=pd.DataFrame()\n",
        "for i in tqdm(range(len(dataset_links))):\n",
        "    tweets=pd.read_csv(dataset_links[i], sep='\\t')\n",
        "    full_data=pd.concat([full_data,tweets],ignore_index=True)\n",
        "full_data.reset_index(inplace=True,drop=True)\n",
        "#the docno will be our tweetID\n",
        "full_data[\"docno\"]=full_data[\"tweetID\"].astype(str)\n",
        "\n",
        "#load the qrels\n",
        "qrels=pd.read_csv(\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/qrels.txt\", sep='\\t',names=['qid','Q0','docno','label'])\n",
        "qrels['docno']=qrels['docno'].astype(str)\n",
        "qrels = qrels[qrels[\"docno\"].isin(full_data[\"docno\"].tolist())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkXv86GFMYJx"
      },
      "source": [
        "Let's load the test queries, rum BM25, then evaluate. Here BM25 is just a baseline that we are going to compare with after using BERT for re-ranking. We selected 10 queries from topics released with EveTAR dataset as a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP92-7BwmDgu"
      },
      "source": [
        "df_queries = pd.read_csv('https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/labs/day9/test_queries.txt', encoding=\"utf-8\",sep='\\t')\n",
        "df_queries[\"query\"]=df_queries[\"query\"].apply(preprocess)\n",
        "# print the test queries\n",
        "df_queries\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0uOViEeRqk9"
      },
      "source": [
        "#intialize our BM25 model to get the top 1000 potentially relevant documents\n",
        "bm25_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"},num_results=1000)\n",
        "#retrieve potentially relevant documents for each query in our test queries\n",
        "bm25_res=bm25_retr.transform(df_queries)\n",
        "#evaluate the performance given the qrels released with EveTAR dataset\n",
        "bm25_eval = pt.Utils.evaluate(bm25_res,qrels[['qid','docno','label']],metrics=[\"map\", \"P\"])\n",
        "bm25_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfT-xpdo8WBv"
      },
      "source": [
        "## **Building the Relevance classifier**\n",
        "\n",
        "Here, we are building a model that takes a query-document pair as an input, feed them to bert, pass the cls embedding to a classification layer, then output a probability score that is between 0 and 1. This score measures how much the document is relevant to the query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAChFH6AN1C"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch import nn\n",
        "\n",
        "class RelevanceClassifier(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    create a RelevanceClassifier model, that can be used to classify text pairs into relevant/non-relevant\n",
        "    This class adds classification layer on top of BERT model\n",
        "    The input is sequence text pair (query & document)\n",
        "    The output is the relevance score, i.e., how much the document is relevant to the query as a score between 0 and 1\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    :param model_name: The name of the BERT model that will be used to get the pair embedding\n",
        "    :param freeze_bert: This Flag is used to allow/(not allow) changing in weights of BERT layers during the training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, freeze_bert=False):\n",
        "\n",
        "        super(RelevanceClassifier, self).__init__()\n",
        "\n",
        "        # load the bert model by its name\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # relu activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768, 2)\n",
        "\n",
        "        # use softmax activation function to give probability distribution\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters():\n",
        "                p.requires_grad = False  # turn off the weight changing\n",
        "\n",
        "    \"\"\"\n",
        "    Define the forward pass for the model\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      :param inputs: the input contains input_ids, attention mask,and token type ids of the input text pair\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      probability scores for relevant & non relevant classes\n",
        "      \"\"\"\n",
        "    # def forward(self, input_ids, attention_mask):\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "\n",
        "        # pass the inputs to the BERT model\n",
        "        _, cls_emb = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=False,\n",
        "        )\n",
        "\n",
        "        # pass the embedding to the classification layer\n",
        "        x = self.fc1(cls_emb)\n",
        "\n",
        "        # apply softmax activation and get the probabilities for each class\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j07m5sD5ZzXZ"
      },
      "source": [
        "## **Create the dataset class**\n",
        " \n",
        "In order to encode data in batches and efficiently, we need to define a class that encodes a given sequence pair and return its input_ids\", attention_mask, and token_type_ids among other needed information like docno and query id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TODqQp40rnkh"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class EvetarDataset(Dataset):\n",
        "    \"\"\"\n",
        "    create a class for Evetar dataset so that for a given query-document pair, it encodes this pair, then it returns \n",
        "    the encoding along with needed ids.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    :param queries: text of query\n",
        "    :param query_ids: ids of the queries\n",
        "    :param documents: text of document\n",
        "    :param document_ids: id of the documents\n",
        "    :param labels: relevance score. It is one for relevant pair and 0 for non-relevant\n",
        "    :param tokenizer: the bert tokenizer that will perform the encoding work\n",
        "    :param max_len: maximum allowed length as an input for bert model\n",
        "    \"\"\"\n",
        "    def __init__(self, queries, query_ids, documents, document_ids, labels, tokenizer, max_len):\n",
        "        self.labels = labels\n",
        "        self.queries = queries\n",
        "        self.documents = documents\n",
        "        self.query_ids = query_ids\n",
        "        self.document_ids = document_ids\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    # return the encoding for a given dataset item, i.e., encode the query-document pair\n",
        "    def __getitem__(self, item):\n",
        "        query = str(self.queries[item])\n",
        "        document = str(self.documents[item])\n",
        "        query_id = str(self.query_ids[item])\n",
        "        document_id = str(self.document_ids[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            query,\n",
        "            document,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            # pad_to_max_length=True,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # here we keep the text and ids of the query and documents to be used later in re-ranking\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"query_id\": query_id,\n",
        "            \"document\": document,\n",
        "            \"document_id\": document_id,\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"token_type_ids\": encoding[\"token_type_ids\"].flatten(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# create data loader that will split the dataset into batches\n",
        "def create_data_loader(\n",
        "    queries, query_ids, documents, document_ids, labels, tokenizer, max_len, batch_size\n",
        "):\n",
        "    ds = EvetarDataset(queries, query_ids, documents, document_ids, labels, tokenizer, max_len)\n",
        "\n",
        "    return DataLoader(ds, batch_size=batch_size, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoReyOVyUXEF"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    \"\"\"\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    \"\"\"\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVw4nuEvc5j"
      },
      "source": [
        "## **Design the training function**\n",
        "\n",
        "Here we implement the training function to fine-tune bert for our task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbzAdPi1TyKP"
      },
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    \"\"\"\n",
        "    Performs one training epoch using the bert model and the provided dataloader\n",
        "    :param model: the model to use during training (classification layer on top of bert)\n",
        "    :param data_loader: data loader to get the data in batches\n",
        "    :param optimizer: what optimizer to use in order to reduce the error rate while training the neural networks\n",
        "    :param device: GPU or Cpu\n",
        "    :param scheduler: Scheduler to adjust the learning rate during training\n",
        "    :param n_examples: total number of training examples\n",
        "    \"\"\"\n",
        "\n",
        "    # to compute execution time\n",
        "    t0 = time.time()\n",
        "\n",
        "    # set the bert model in training mode, i.e., weights will be updated\n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    y_test = np.array([], dtype=int)  # the real output\n",
        "    y_pred = np.array([], dtype=int) # the predicted output\n",
        "\n",
        "    for step, batch in enumerate(data_loader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print(\"  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.\".format(step, len(data_loader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from dataloader.\n",
        "        label = batch[\"label\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # the output here for each training example is two values. One represent the score for relevance class\n",
        "        # and the other is the score of the non-relevant class\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        # compute the loss between the predicted output and the real output\n",
        "        loss = loss_fn(outputs, label)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == label)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "        y_test = np.append(y_test, label.cpu().numpy())\n",
        "        y_pred = np.append(y_pred, preds.cpu().numpy())\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  correct_predictions: {0:.2f}\".format(correct_predictions.double()))\n",
        "    print(\"  number of examples: {0:.2f}\".format(n_examples))\n",
        "    print(\"  Accuracy : {0:.2f}\".format(correct_predictions.double() / n_examples))\n",
        "    print(\"  Average training loss: {0:.2f}\".format(np.mean(losses)))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "    y_test = np.array(y_test)\n",
        "    y_pred = np.array(y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return accuracy, np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaVBQYTxv4Pw"
      },
      "source": [
        "Here, we provide an evaluation function that could help when we have train, dev, and test data. Since, in our case, we have only training and testing data we will not use this function. We are just providing this function as a reference for you in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NG3sjl1VPXM"
      },
      "source": [
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    print(\"Running Evaluation...\")\n",
        "    t0 = time.time()  \n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    model = model.eval()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            label = batch[\"label\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            loss = loss_fn(outputs, label)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == label)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    print(\"  correct_predictions: {0:.2f}\".format(correct_predictions.double()))\n",
        "    print(\"  n_examples: {0:.2f}\".format(n_examples))\n",
        "    print(\"  Accuracy: {0:.2f}\".format(correct_predictions.double() / n_examples))\n",
        "    print(\"  Average Validation loss: {0:.2f}\".format(np.mean(losses)))\n",
        "    print(\"  Evaluation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    print(\"\")\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bvvfXCGwfzT"
      },
      "source": [
        "## **Design the test function** \n",
        "\n",
        "\n",
        "This function gives the predictions of our fine-tuned model. More specifically, it gives the id of each query and document along with the relevance score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p044kT2tVwnA"
      },
      "source": [
        "def get_predictions(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Feed the test dataloader to the fine-tuned bert model and gives back the prediction results.\n",
        "    :param model: the model to use during training (classification layer on top of bert)\n",
        "    :param data_loader: test data loader to get the data in batches\n",
        "    :param device: GPU or Cpu\n",
        "    \"\"\"\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    model = model.eval()\n",
        "\n",
        "    predictions = []  # predicted labels (0s or 1s)\n",
        "    prediction_probs = [] # probability score of the relevance class\n",
        "    labels = []\n",
        "    query_ids = []\n",
        "    document_ids = []\n",
        "    queries = []\n",
        "    documents = []\n",
        "    indices = torch.tensor([1]).to(device)\n",
        "\n",
        "    with torch.no_grad(): # don't update gradients\n",
        "        for batch in data_loader:\n",
        "\n",
        "            # encoding = batch[\"encoding\"]\n",
        "            query = batch[\"query\"]\n",
        "            query_id = batch[\"query_id\"]\n",
        "            document = batch[\"document\"]\n",
        "            document_id = batch[\"document_id\"]\n",
        "            label = batch[\"label\"].to(device)\n",
        "\n",
        "            # outputs = model(**encoding)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # choose the neuron that predics the relevance score , reference: https://pytorch.org/docs/stable/generated/torch.index_select.html\n",
        "            probs = torch.index_select(outputs, dim=1, index=indices)\n",
        "\n",
        "\n",
        "            labels.extend(label)\n",
        "            prediction_probs.extend(probs.flatten().tolist())\n",
        "            predictions.extend(preds)\n",
        "            queries.extend(query)\n",
        "            documents.extend(document)\n",
        "            document_ids.extend(document_id)\n",
        "            query_ids.extend(query_id)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    labels = torch.stack(labels).cpu()\n",
        "    # prediction_probs = prediction_probs.numpy()\n",
        "    # prediction_probs = torch.stack(prediction_probs).cpu().tolist\n",
        "    return queries, query_ids, documents, document_ids, predictions, prediction_probs, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s13cacw1PjO"
      },
      "source": [
        "## **Load the training set**\n",
        "\n",
        "In this step, we are loading the training set from our GitHub repository. The training set has 10 positive and negative examples for each tweet, i.e., 10 for class 0 and 10 for class 1. The training set also contains the ids of each document and query. While the positive examples were chosen from qrels file, negative examples were chosen from the top retrieved documents by BM25 and given that they are not considered already as positive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgwDPmdb5VcK"
      },
      "source": [
        "# prepare training data\n",
        "df_train = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/labs/day9/training_data_with_text_for_query_and_document.txt\", \n",
        "    encoding=\"utf-8\", sep=\"\\t\"\n",
        ")\n",
        "\n",
        "df_train = df_train.sample(frac=1) # Just shuffle the data\n",
        "x_train_query = df_train[\"query\"].values\n",
        "x_train_query_id = df_train[\"qid\"].values\n",
        "x_train_documents = df_train[\"document\"].values\n",
        "x_train_document_ids = df_train[\"docno\"].values\n",
        "y_train_label = df_train[\"label\"].values\n",
        "\n",
        "TRAIN_LENGTH = len(x_train_query)\n",
        "print(\"train size \", TRAIN_LENGTH)\n",
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkhaMb7x2CNB"
      },
      "source": [
        "## **Load the test data**\n",
        "\n",
        "The test data is just output of one of the exercises in previous sessions. Shortly, for each tweet, we have 1000 document retrieved by BM25. The goal is to re-rank those documents so that the most relevant ones become on top. The testing set attributes are similar to the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClCwUK9e9BCX"
      },
      "source": [
        "\n",
        "# prepare testing data\n",
        "#here we have two testing set\n",
        "# the first one contains the top 1000 document retrieved by bm25, \n",
        "# the second contains the top 100 only\n",
        "df_test_top1000 = pd.read_csv(\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/labs/day9/bm25_top_1000_with_text.txt\", encoding=\"utf-8\", sep=\"\\t\")\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/labs/day9/bm25_top_100_with_text.txt\", encoding=\"utf-8\", sep=\"\\t\")\n",
        "\n",
        "\n",
        "x_test_query = df_test[\"query\"].values\n",
        "x_test_query_id = df_test[\"qid\"].values\n",
        "x_test_document = df_test[\"document\"].values\n",
        "x_test_document_ids = df_test[\"docno\"].values\n",
        "y_test_labels = [0] * len(x_test_query)\n",
        "\n",
        "\n",
        "TEST_LENGTH = len(x_test_query)\n",
        "\n",
        "print(\"test size \", TEST_LENGTH)\n",
        "df_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph2rpqmb2uI6"
      },
      "source": [
        "## **Setting the hyper parameters**\n",
        "\n",
        "In this section, we are setting the hyperparameter for BERT model like the batch size, learning rate, maximum length, and number of epochs. Moreover, we choose what model to use, define optimizer and scheduler. In addition, we create the train and test data loaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnVFxcjaK3Au"
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Here, I am listing some Arabic bert models that you can try\n",
        "MARBERT = \"UBC-NLP/MARBERT\"\n",
        "ARBERT = \"UBC-NLP/ARBERT\"\n",
        "CAMeLBERT_mix = \"CAMeL-Lab/bert-base-camelbert-mix\"\n",
        "Arabic_BERT = \"asafaya/bert-base-arabic\"\n",
        "QARiB = \"qarib/bert-base-qarib\"\n",
        "AraBERT = \"aubmindlab/bert-base-arabertv02\"\n",
        "\n",
        "\n",
        "\n",
        "model_name = AraBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# set the random seed to replicate results\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "batch_size = 16  # try other batch sizes like 32, or 8\n",
        "num_epochs = 3  # you can try 2, or 5\n",
        "MAX_LEN = 180\n",
        "learning_rate = 2e-5  # you can try 3e-5, 5e-5\n",
        "\n",
        "\n",
        "# Create train and test dataloaders\n",
        "\n",
        "train_data_loader = create_data_loader(\n",
        "    queries=x_train_query,\n",
        "    query_ids=x_train_query_id,\n",
        "    documents=x_train_documents,\n",
        "    document_ids=x_train_document_ids,\n",
        "    labels=y_train_label,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "test_data_loader = create_data_loader(\n",
        "    queries=x_test_query,\n",
        "    query_ids=x_test_query_id,\n",
        "    documents=x_test_document,\n",
        "    document_ids=x_test_document_ids,\n",
        "    labels=y_test_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "# initialize the model\n",
        "model = RelevanceClassifier(model_name=model_name, freeze_bert=False)\n",
        "model = model.to(device)\n",
        "\n",
        "# create the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False)\n",
        "\n",
        "total_steps = len(train_data_loader) * num_epochs\n",
        "\n",
        "#   10% of train data for warm-up\n",
        "warmup_steps = math.ceil(len(train_data_loader) * num_epochs * 0.05)\n",
        "#  Warmup steps are just a few updates with low learning rate before / at the beginning of training. \n",
        "#  After this warmup, you use the regular learning rate (schedule) to train your model to convergence.\n",
        "# The idea that this helps your network to slowly adapt to the data intuitively makes sense. \n",
        "#  However, theoretically, the main reason for warmup steps is to allow adaptive optimisers (e.g. Adam, RMSProp, ...) \n",
        "# to compute correct statistics of the gradients. Check this: https://datascience.stackexchange.com/questions/55991/in-the-context-of-deep-learning-what-is-training-warmup-steps\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNNH6aJ8E9k7"
      },
      "source": [
        "## **Training the model**\n",
        "The last step is just to invoke train_epoch function and pass the required parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-PQ78QGA8dq"
      },
      "source": [
        "# start training\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model, train_data_loader, loss_fn, optimizer, device, scheduler, TRAIN_LENGTH,\n",
        "    )\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teANsinvFO9F"
      },
      "source": [
        "##**Save and load the fine-tuned model**\n",
        "\n",
        "After fine-tuning AraBERT model, we want to save the model for future uses. Thus, save the training time. We just need to provide the save path for save method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZY97sU1bMUH"
      },
      "source": [
        "# save the trained model\n",
        "\n",
        "model_file_path = \"trained_model.bin\"\n",
        "torch.save(model.state_dict(), model_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVrgmhTxFk_R"
      },
      "source": [
        "For loading, we just need to initiliaze the model architecture, then load the weights using the load mehtod."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf0yPXCobnMX"
      },
      "source": [
        "# Load the saved model\n",
        "model_name = AraBERT\n",
        "model = RelevanceClassifier(model_name=model_name, freeze_bert=False)\n",
        "model.load_state_dict(torch.load(model_file_path, map_location=torch.device(device)))\n",
        "model = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-CVye_8F6-h"
      },
      "source": [
        "## **Reranking**\n",
        "\n",
        "Now, the model is ready for testing phase. We just need to provide the query-document pair. Then, the model will predict the relevance score. After that, we re-rank documents based on this score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7rXYNTKG96e"
      },
      "source": [
        "# get the predictions scores using the fine-tuned model for the test set\n",
        "t0 = time.time()\n",
        "queries, query_ids, documents, document_ids, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader, device)\n",
        "print(\"Prediction of test set took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ5PAxfEcKCZ"
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "# Create a new dataframe that will contain the re-ranked pairs\n",
        "df = pd.DataFrame()\n",
        "df['qid']= query_ids             \n",
        "df[\"query\"] = queries\n",
        "df['docno']= document_ids             \n",
        "df['document'] = documents         \n",
        "df['score']= y_pred_probs \n",
        "df[\"bm25_rank\"] = df_test[\"bm25_rank\"]  \n",
        "\n",
        "processed = []\n",
        "\n",
        "df_result = pd.DataFrame()\n",
        "for i, elem in df.iterrows():\n",
        "    query_id = elem['qid']\n",
        "    # process all documents paired with this query_id at once\n",
        "    if query_id in processed:\n",
        "        continue\n",
        "    processed.append(query_id)\n",
        "\n",
        "    df_one_tweet = df[df['qid'] == query_id] # get all rows that has query_id value\n",
        "\n",
        "    # here we re-rank pairs based on the score column, tie is broken by docno\n",
        "    df_one_tweet = df_one_tweet.sort_values(by=[\"score\", \"docno\"], ascending=False)\n",
        "\n",
        "\n",
        "    # give a new rank for the re-ranked pairs\n",
        "    k = 0\n",
        "    for j, elem in df_one_tweet.iterrows():\n",
        "        new_row = {\n",
        "            'qid': elem['qid'],\n",
        "            'query': elem['query'],\n",
        "            'docno': elem['docno'],\n",
        "            'document': elem['document'],\n",
        "            'score': elem['score'],\n",
        "            'bm25_rank': elem[\"bm25_rank\"],\n",
        "            'rank': k,\n",
        "        }\n",
        "        # append the new row to the final dataframe\n",
        "        df_result = df_result.append(new_row, ignore_index=True)\n",
        "        k = k + 1\n",
        "\n",
        "\n",
        "print(\"  Re-ranking took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3EX9eFxiMlf"
      },
      "source": [
        "df_result[\"rank\"] = df_result[\"rank\"].astype(int)\n",
        "df_result[\"bm25_rank\"] = df_result[\"bm25_rank\"].astype(int)\n",
        "df_result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTd6mVaphw2C"
      },
      "source": [
        "save_path = \"reranked_test_set_top100.csv\"\n",
        "df_result.to_csv(save_path,encoding=\"utf-8\", sep=\"\\t\", index=False)  \n",
        "print(\"Output is saved into \", save_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynJhKZTCR321"
      },
      "source": [
        "# evaluate the re-ranked pairs \n",
        "bert_eval = pt.Utils.evaluate(df_result,qrels[['qid','docno','label']],metrics=[\"map\",\"P\"])\n",
        "# evaluate bm25 on the same queries set\n",
        "bm25_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"},num_results=100)\n",
        "# if you want to compare with the top 1000\n",
        "# bm25_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"},num_results=1000)\n",
        "bm25_res=bm25_retr.transform(df_queries)\n",
        "bm25_eval = pt.Utils.evaluate(bm25_res,qrels[['qid','docno','label']],metrics=[\"map\", \"P\"])\n",
        "\n",
        "# put them in one dataframe for comparison purposes\n",
        "df_res = pd.DataFrame({'BM25':pd.Series(bm25_eval),'bert_reranker':pd.Series(bert_eval)}).T\n",
        "df_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTF5GThjZBJO"
      },
      "source": [
        "Comparison if we rerank the top 1000 documents\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1SlGh913--jfrYmJkAQY6zuADRLfXLIea)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FVdZjX5vHMg"
      },
      "source": [
        "## **Exercise1**\n",
        "\n",
        "In the previous work, we provide documents for BERT as they are. However, the document contains urls and other special characters such as emojies that may confuse the bert model. \n",
        "\n",
        "1. Your job is to perform cleaning on the documents before feeding them to BERT. This should be done on both training and testing set.\n",
        "2. Perform fine-tuning to the model. Then re-rank the test set and evaluate. \n",
        "3. Compare with the model before applying cleaning. Does it make a difference?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKz4fdjBP1dg"
      },
      "source": [
        "# write your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_fMCB2FXIG"
      },
      "source": [
        "## **References**\n",
        "\n",
        "\n",
        "\n",
        "*   [BERT Fine-Tuning Tutorial with PyTorch.](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "\n",
        "\n",
        "*   [BERT in hugging face](https://huggingface.co/transformers/model_doc/bert.html)\n",
        "\n",
        "\n",
        "*   [what is training warmup steps](https://datascience.stackexchange.com/questions/55991/in-the-context-of-deep-learning-what-is-training-warmup-steps)"
      ]
    }
  ]
}