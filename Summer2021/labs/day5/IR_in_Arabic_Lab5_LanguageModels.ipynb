{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-in-Arabic_Lab5-LanguageModels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telsayed/IR-in-Arabic/blob/master/Summer2021/labs/day5/IR_in_Arabic_Lab5_LanguageModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzUxXUVMVVU"
      },
      "source": [
        "\n",
        "\n",
        "# **IR in Arabic** - Summer 2021 lab day5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjTczIiMctt"
      },
      "source": [
        "This is one of a series of Colab notebooks created for the **IR in Arabic** course. It demonstrates how we can perform ranked retrieval using a language model and evaluate the output of multiple retrieval models.\n",
        "\n",
        "The **learning outcomes** of the this notebook are:\n",
        "\n",
        "\n",
        "*   Retrieval using a language model with Jelineck-Mercer smoothing.\n",
        "*   Evaluate and compare the results of multiple retrieval models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkS6LLkX6HHV"
      },
      "source": [
        "### **Setup**\n",
        "We will first install Pyterrier as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sKgPMd_-gU"
      },
      "source": [
        "#install the Pyterrier framework\n",
        "!pip install python-terrier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIzygfXzAILT"
      },
      "source": [
        "The next step is to initialise PyTerrier. This is performed using PyTerrier's init() method. The init() method is needed as PyTerrier must download Terrier's jar file and start the Java virtual machine. We prevent init() from being called more than once by checking started()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAChFH6AN1C"
      },
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMSW5eI-TQGX"
      },
      "source": [
        "Another library that we need for this lab is Arabic-Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENifjmGeTf6Q"
      },
      "source": [
        "#install the Arabic stop words library\n",
        "!pip install Arabic-Stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnwlwDLJT48p"
      },
      "source": [
        "We will import all the python libraries needed for this lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os60lty5UD-s"
      },
      "source": [
        "#we need to import the following libraries.\n",
        "import pandas as pd\n",
        "#to display the full text on the notebook without truncation\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "import numpy as np\n",
        "import re\n",
        "from snowballstemmer import stemmer\n",
        "from tqdm import tqdm\n",
        "import arabicstopwords.arabicstopwords as stp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkxBHwFE8LH6"
      },
      "source": [
        "We will prepare our helper functions for removing stop words, normalize, and stemming which we will use to process our queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KWReIAF8Rxz"
      },
      "source": [
        "#removing Stop Words function\n",
        "def remove_stop_words(sentence):\n",
        "    terms=[]\n",
        "    stopWords= set(stp.stopwords_list())\n",
        "    for term in sentence.split() : \n",
        "        if term not in stopWords :\n",
        "           terms.append(term)\n",
        "    return \" \".join(terms)\n",
        "\n",
        "#a function to normalize the tweets\n",
        "def normalize(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    return(text)\n",
        "\n",
        "#define the stemming function\n",
        "ar_stemmer = stemmer(\"arabic\")\n",
        "def stem(sentence):\n",
        "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])\n",
        "  \n",
        "\n",
        "# perform all preprocessing steps\n",
        "def preprocess(sentence):\n",
        "  # apply preprocessing steps on the given sentence\n",
        "  sentence =remove_stop_words(sentence)\n",
        "  sentence =normalize(sentence)\n",
        "  sentence =stem(sentence)\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ170aUr7b2D"
      },
      "source": [
        "We will use our indexed **EveTAR** dataset. The index is uploaded in our Github repository so we will access it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkHkRSGz7sfF"
      },
      "source": [
        "%rm -rf IR-in-Arabic\n",
        "%rm -rf evetarIndex\n",
        "!git clone https://github.com/telsayed/IR-in-Arabic.git \n",
        "!unzip IR-in-Arabic/Summer2021/data/EveTAR/evetarIndex.zip -d evetarIndex\n",
        "!ls evetarIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkznRg8-VXx"
      },
      "source": [
        "Next, we will load our index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuqjM544-ZKx"
      },
      "source": [
        "#we will load the index\n",
        "index_ref = pt.IndexRef.of(\"./evetarIndex/data.properties\")\n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgf3qxzfvZMR"
      },
      "source": [
        "Let's load our collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uylSh8f5vO0m"
      },
      "source": [
        "dataset_links=[\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-01.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-02.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-03.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-04.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-05.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-06.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-07.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-08.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-09.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-10.txt\"]\n",
        "\n",
        "full_data=pd.DataFrame()\n",
        "for i in tqdm(range(len(dataset_links))):\n",
        "    tweets=pd.read_csv(dataset_links[i], sep='\\t')\n",
        "    full_data=pd.concat([full_data,tweets],ignore_index=True)\n",
        "full_data.reset_index(inplace=True,drop=True)\n",
        "#the docno will be our tweetID\n",
        "full_data[\"docno\"]=full_data[\"tweetID\"].astype(str)\n",
        "full_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIL0VhkxXi6z"
      },
      "source": [
        "We will use load queries that are already defined and released with EveTAR dataset and process using the same processing steps we did when we indexed EveTAR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAjn8zFGW4x0"
      },
      "source": [
        "#read the queries file from Github\n",
        "topics=pd.read_csv(\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/topics.txt\", sep='\\t',names=['data'])\n",
        "queries=[]\n",
        "qid=[]\n",
        "#we will get the queries and their ids from the topics file\n",
        "for i in range(len(topics)):\n",
        "    splitted=topics[\"data\"][i:i+1][i].split(\" \")\n",
        "    if splitted[0]==\"<title>\":\n",
        "       queries.append(' '.join(splitted[1:]))\n",
        "    if splitted[0]==\"<num>\":\n",
        "       qid.append(splitted[2])\n",
        "queriesDF=pd.DataFrame() \n",
        "#the queries datframe should have qid and query columns to retrieve using PyTerrier      \n",
        "queriesDF[\"qid\"]=qid\n",
        "queriesDF[\"raw_query\"]=queries\n",
        "#remove the stopwords from queries, do normalization, and apply stemming \n",
        "queriesDF[\"query\"]=queriesDF[\"raw_query\"].apply(preprocess)\n",
        "queriesDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgIpOhpCL_QO"
      },
      "source": [
        "**Retrieval with a language model using Jelinek-Mercer smoothing**\n",
        "\n",
        "\n",
        "We will use BatchRetrieve Pyterrier class for retrieval and  **Hiemstra LM weighting model** which supports **Jelinek-Mercer smoothing** as the weighting model. You can check the weighting models supported by PyTerrier [here](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html).\n",
        "\n",
        "Lambda parameter for Jelinek-Mercer smoothing is set to 0.15 by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3__M78_IttOa"
      },
      "source": [
        "#set up our retieval model by specifing Hiemstra_LM as wmodel and limiting the number of results for each query top 10 documents\n",
        "JM_retr = pt.BatchRetrieve(index,wmodel=\"Hiemstra_LM\",num_results=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jEgittRyN4a"
      },
      "source": [
        "Let's try a single query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOvFVcj5tz9b"
      },
      "source": [
        "query=\"مباراة العراق وكوريا الجنوبية في نصف نهائي كأس آسيا\"\n",
        "#we need to process the query also as we did for documents\n",
        "query = preprocess(query)\n",
        "#we will call the search function using our retrieval model we set up above\n",
        "results=JM_retr.search(query)\n",
        "if len(results)==0:\n",
        "   print(\"There are no relevant documents for your selected query.\")\n",
        "else:\n",
        "   print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyQiq395t6Gd"
      },
      "source": [
        "#Let's check the tweets text for the top 5 retrieved tweets\n",
        "full_data[full_data['docno'].isin(results['docno'].loc[0:4].tolist())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knS11Z9XyX4A"
      },
      "source": [
        "Let's update **Lambda** and set it to 0.95\n",
        "\n",
        "* **High value of λ:**“conjunctive-like” search –tends to retrieve documents containing all query words.\n",
        "*  **Low value of λ:** more disjunctive, suitable for long queries\n",
        "* Correctly setting λ is very important for good performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fV_jq-IkToT"
      },
      "source": [
        "#set up our retieval model by specifing Hiemstra_LM as wmodel and limiting the number of results for each query top 10 documents\n",
        "JM_retr_highLambda = pt.BatchRetrieve(index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.95},num_results=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O371L3r0NBw8"
      },
      "source": [
        "query=\"مباراة العراق وكوريا الجنوبية في نصف نهائي كأس آسيا\"\n",
        "#we need to process the query also as we did for documents\n",
        "query = preprocess(query)\n",
        "#we will call the search function using our retrieval model we set up above\n",
        "results=JM_retr_highLambda.search(query)\n",
        "if len(results)==0:\n",
        "   print(\"There are no relevant documents for your selected query.\")\n",
        "else:\n",
        "   print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5FERyw5KHXo"
      },
      "source": [
        "Let's check the text of top 5 retrieved tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iKYdq6FH92S"
      },
      "source": [
        "full_data[full_data['docno'].isin(results['docno'].loc[0:4].tolist())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9P61FAxNehR"
      },
      "source": [
        "We can retrieve the relevant documents to a set of queries. We will use the set of queries we prepared earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kba9gPG0z6Vc"
      },
      "source": [
        "#RetrIEve using the Jelinek-Mercer smoothing where lambda=0.15 (default)\n",
        "JM_res=JM_retr.transform(queriesDF)\n",
        "JM_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5He7zRGNpYP"
      },
      "source": [
        "#Retreive using the Jelinek-Mercer smoothing where lambda=0.95\n",
        "JM_highLambda_res=JM_retr_highLambda.transform(queriesDF)\n",
        "JM_highLambda_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdUzSE0mPLTP"
      },
      "source": [
        "### **Evaluating our results** \n",
        "To evaluate the results we need to load our qrels (relevance judgements) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k28ejDvJRbx5"
      },
      "source": [
        "qrels=pd.read_csv(\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/qrels.txt\", sep='\\t',names=['qid','Q0','docno','label'])\n",
        "qrels['docno']=qrels['docno'].astype(str)\n",
        "qrels = qrels[qrels[\"docno\"].isin(full_data[\"docno\"].tolist())]\n",
        "qrels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfBwU7oXyPLB"
      },
      "source": [
        "To evaluate our results we can use Pyterrier Utils.evaluate function. This function take the results and the qrels dataframe containing three columns which are **qid, docno, label.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCTYk9AZ0Rd1"
      },
      "source": [
        "eval = pt.Utils.evaluate(JM_res,qrels[['qid','docno','label']],metrics=[\"map\",\"recall\",\"P\"])\n",
        "eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QdMp3pb0uK2"
      },
      "source": [
        "eval = pt.Utils.evaluate(JM_highLambda_res,qrels[['qid','docno','label']],metrics=[\"map\",\"recall\",\"P\"])\n",
        "eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTzy3w9mLvZi"
      },
      "source": [
        "### **How to compare between different retrieval models using PyTerrier.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82OLTOqZ3BtO"
      },
      "source": [
        "Pyterrier make it easy for us to compare between different retrieval models. Let's compare between different JM models with 3 different values of lambda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "822_VDvf3BLL"
      },
      "source": [
        "JM_retr = pt.BatchRetrieve(index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.15},num_results=1000)\n",
        "JM_retr_highLambda = pt.BatchRetrieve(index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.95},num_results=1000)\n",
        "JM_retr_lowLambda = pt.BatchRetrieve(index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.01},num_results=1000)\n",
        "#call pt.Experiment\n",
        "pt.Experiment(\n",
        "[JM_retr ,JM_retr_highLambda, JM_retr_lowLambda],\n",
        "queriesDF,\n",
        "qrels,\n",
        "eval_metrics=[\"map\", \"P\"], \n",
        "names=[\"JM_retr\",\"JM_retr_highLambda\",\"JM_retr_lowLambda\"]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMS5QB4Ldxm0"
      },
      "source": [
        "Pyterrier make it easy for us to compare between different retrieval models. Let's compare between the previous lab models (BM25, TF_IDF) and today's model JM.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb8JIOAOeB_W"
      },
      "source": [
        "JM_retr = pt.BatchRetrieve(index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.15},num_results=1000)\n",
        "JM_retr_highLambda = pt.BatchRetrieve(index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.95},num_results=1000)\n",
        "bm25_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"},num_results=1000)\n",
        "tfidf_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"TF_IDF\"},num_results=1000)\n",
        "\n",
        "pt.Experiment(\n",
        "[JM_retr,JM_retr_highLambda ,bm25_retr, tfidf_retr],\n",
        "queriesDF,\n",
        "qrels,\n",
        "eval_metrics=[\"map\", \"P\"], \n",
        "names=[\"JM_retr\",\"JM_retr_highLambda\",\"bm25_retr\",\"tfidf_retr\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLmX1ZDgAo1c"
      },
      "source": [
        "Other useful parameters:\n",
        "1.   **dataframe(bool)**: If True return results as a dataframe. Else as a dictionary of dictionaries. Default=True.\n",
        "2.   **perquery(bool)**: If True return each metric for each query, else return mean metricsacross all queries. Default=False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1yl-jH_iD_"
      },
      "source": [
        "pt.Experiment(\n",
        "[JM_retr,JM_retr_highLambda ,bm25_retr, tfidf_retr],\n",
        "queriesDF,\n",
        "qrels,\n",
        "eval_metrics=[\"map\", \"P\"], \n",
        "names=[\"JM_retr\",\"JM_retr_highLambda\",\"bm25_retr\",\"tfidf_retr\"],\n",
        "perquery=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMUvVDSW_6zn"
      },
      "source": [
        "pt.Experiment(\n",
        "[JM_retr,JM_retr_highLambda ,bm25_retr, tfidf_retr],\n",
        "queriesDF,\n",
        "qrels,\n",
        "eval_metrics=[\"map\", \"P\"], \n",
        "names=[\"JM_retr\",\"JM_retr_highLambda\",\"bm25_retr\",\"tfidf_retr\"],\n",
        "dataframe=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ9IN8xpkJzg"
      },
      "source": [
        "## **Exercise1**\n",
        "Use the first 10 queries from the set of our queries to retrieve the top 100 potentially relevant documents using both BM25 and language model with JM smoothing by setting Lambda to 0.8. Compare between the results in terms of map only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuXoF_5AWzJq"
      },
      "source": [
        "#add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6590-CeSgSOr"
      },
      "source": [
        "## **Exercise 2**\n",
        "Given the following queries:\n",
        "\n",
        "['E14' 'E48' 'E36' 'E58' 'E19' 'E63' 'E30' 'E27' 'E39' 'E21']\n",
        "\n",
        "1. Retrieve the top 1000 relevant documents using the language model with JM smoothing by setting lambda to 0.9.\n",
        "2. Retrieve the text for both queries and documents and make them into one dataframe.\n",
        "3. Save the resulted dataframe into a text file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgRvwa2dUdBb"
      },
      "source": [
        "selected_queries = ['E14','E48', 'E36', 'E58', 'E19', 'E63', 'E30', 'E27', 'E39', 'E21']\n",
        "# write your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7g6YHetkMy8"
      },
      "source": [
        "### **Exercise 3**\n",
        "\n",
        "Explore the weighting models supported by PyTerrier [here](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html). Choose multiple models to retrieve the relevant tweets for the full set of queries and compare between the results in terms of map and precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_fMCB2FXIG"
      },
      "source": [
        "## **References**\n",
        "\n",
        "\n",
        "* [PyTerrier  retrieval and evaluation notebook](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/retrieval_and_evaluation.ipynb).\n",
        "*   [PyTerrier documentation.](https://pyterrier.readthedocs.io/_/downloads/en/latest/pdf/)\n",
        "\n"
      ]
    }
  ]
}