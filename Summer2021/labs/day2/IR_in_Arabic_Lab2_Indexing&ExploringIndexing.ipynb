{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-in-Arabic_Lab2-Indexing&ExploringIndexing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telsayed/IR-in-Arabic/blob/master/Summer2021/labs/day2/IR_in_Arabic_Lab2_Indexing%26ExploringIndexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzUxXUVMVVU"
      },
      "source": [
        "\n",
        "\n",
        "# **IR in Arabic** - Summer 2021 lab notebook 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjTczIiMctt"
      },
      "source": [
        "This is one of a series of Colab notebooks created for the **IR in Arabic** course. It demonstrates how we can index a collection, and how to access an index to visualize some index analysis.\n",
        "\n",
        "The **learning outcomes** of the this notebook are:\n",
        "\n",
        "\n",
        "*   PyTerrier setup.\n",
        "*   Preprocessing.\n",
        "*   Indexing a collection.\n",
        "*   Accessing and exploring the index.\n",
        "\n",
        "What is PyTerrier?\n",
        "\n",
        "**[PyTerrier](https://pyterrier.readthedocs.io/en/latest/)** is a Python framework, but uses the underlying [Terrier information retrieval](http://terrier.org/) toolkit for many indexing and retrieval operations. While PyTerrier was new in 2020, Terrier is written in Java and has a long history dating back to 2001. PyTerrier makes it easy to perform IR experiments in Python, but using the mature Terrier platform for the expensive indexing and retrieval operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkS6LLkX6HHV"
      },
      "source": [
        "### **Setup**\n",
        "We will first install Pyterrier as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sKgPMd_-gU"
      },
      "source": [
        "#install the Pyterrier framework\n",
        "!pip install python-terrier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIzygfXzAILT"
      },
      "source": [
        "The next step is to initialise PyTerrier. This is performed using PyTerrier's init() method. The init() method is needed as PyTerrier must download Terrier's jar file and start the Java virtual machine. We prevent init() from being called more than once by checking started()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAChFH6AN1C"
      },
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMSW5eI-TQGX"
      },
      "source": [
        "Another library that we need for this lab is Arabic-Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENifjmGeTf6Q"
      },
      "source": [
        "#install the Arabic stop words library\n",
        "!pip install Arabic-Stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvXs6SpBZ6mD"
      },
      "source": [
        "We will import all the python libraries needed for this lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3cU_TmDZ_Jf"
      },
      "source": [
        "#we need to import the following libraries.\n",
        "import pandas as pd\n",
        "#to display the full text on the notebook without truncation\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "import re\n",
        "from snowballstemmer import stemmer\n",
        "import arabicstopwords.arabicstopwords as stp\n",
        "#make your loops show a smart progress meter \n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEFW83xhgeiy"
      },
      "source": [
        "### **What are DataFrames?** \n",
        "[Pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html): Two-dimensional, size-mutable, potentially heterogeneous tabular data. Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5o48EoXo_V"
      },
      "source": [
        "#create a new dataframe\n",
        "my_df=pd.DataFrame([[\"Ahmed\",25,50000],[\"Fatima\",35,690000],[\"Nada\",45,460000]],columns=['name','age','salary'])\n",
        "my_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNHjYzKFiHXO"
      },
      "source": [
        "#insert a new row\n",
        "my_df=my_df.append({'name':'Salwa','age':24,'salary':90000},ignore_index=True)\n",
        "my_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhBQoYJZlDM5"
      },
      "source": [
        "#print just name and salary\n",
        "my_df[['name','salary']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL-u3Bu6ihG-"
      },
      "source": [
        "#print the data about people with salary>60000\n",
        "my_df[my_df['salary']>60000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxIxTwp5itXW"
      },
      "source": [
        "#increase the salary of all by 1000\n",
        "def increase_salary(salary):\n",
        "    return salary+1000\n",
        "    \n",
        "my_df[\"salary\"]=my_df[\"salary\"].apply(increase_salary)\n",
        "my_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvYVmK5WOD-"
      },
      "source": [
        "### **Data preparation**\n",
        "We will first create five textual documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoD-lm5-YSLx"
      },
      "source": [
        "docs_df = pd.DataFrame([ [\"d0\", \"هذا هو اليوم الأول من دورة استرجاع المعلومات\"],\n",
        "                        [\"d1\", \"الدورة باللغة العربية للطلاب العرب\"], \n",
        "                        [\"d2\", \"اليوم هو 30 مايو 2021\"], \n",
        "                        [\"d3\", \"نأمل أن تفيد هذه الدورة الطلاب العرب\"],\n",
        "                        [\"d4\", \"هل أنتم سعداء بهذه التجربة؟\"] ],\n",
        "                        columns=[\"docno\", \"raw_text\"])\n",
        "\n",
        "docs_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyKXDJf0p_Lp"
      },
      "source": [
        "Before indexing our data we need to do the following processing steps:\n",
        "\n",
        "\n",
        "1.   **Remove stopwords.**\n",
        "2.   **Normalization.**\n",
        "3.   **Stemming.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB-qA3T9qoBs"
      },
      "source": [
        "\n",
        "Let's remove the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A59dw1U0Z_QU"
      },
      "source": [
        "stp.stopwords_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54iZRAN2lbhX"
      },
      "source": [
        "len(stp.stopwords_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceLmvDiTq6oW"
      },
      "source": [
        "#removing Stop Words function\n",
        "def remove_stopWords(sentence):\n",
        "    terms=[]\n",
        "    stopWords= set(stp.stopwords_list())\n",
        "    for term in sentence.split() : \n",
        "        if term not in stopWords :\n",
        "           terms.append(term)\n",
        "    return \" \".join(terms)\n",
        "\n",
        "docs_df[\"text\"]=docs_df[\"raw_text\"].apply(remove_stopWords)\n",
        "print(\"***************************************************************************documents after removing stopwords*********************************************************************\")\n",
        "docs_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_dkV6D4rgOI"
      },
      "source": [
        "After removing the stopwords the next step is to normalize our documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnQz-uyzrsRd"
      },
      "source": [
        "#a function to normalize the tweets\n",
        "def normalize(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    return(text)\n",
        "\n",
        "docs_df[\"text\"]=docs_df[\"text\"].apply(normalize)\n",
        "print(\"***************************************************************************documents after normalizing*********************************************************************\")\n",
        "docs_df  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZY1yWSgr_jj"
      },
      "source": [
        "The last processing step is to stem the terms in each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJbx-DfWnEIw"
      },
      "source": [
        "#specify that we want to stem arabic text\n",
        "ar_stemmer = stemmer(\"arabic\")\n",
        "#define the stemming function\n",
        "def stem(sentence):\n",
        "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])\n",
        "\n",
        "docs_df['text']=docs_df['text'].apply(stem)\n",
        "print(\"***************************************************************************documents after stemming*********************************************************************\")\n",
        "docs_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA-HvUt_Yedy"
      },
      "source": [
        "Next, we will index the dataframe's documents. The index, with all its data structures, is saved into a directory called **myFirstIndex**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOwBC5Rwt8jk"
      },
      "source": [
        "indexer = pt.DFIndexer(\"./myFirstIndex\", overwrite=True)\n",
        "#as the default is an English tokenizer we will update it by setting it to a non-English tokenizer \"UTFTokenizer\"\n",
        "indexer.setProperty(\"tokeniser\", \"UTFTokeniser\")\n",
        "# index the text, record the docnos as metadata\n",
        "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
        "index_ref.toString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvl7XfAZlOM"
      },
      "source": [
        "### **Explore the index**\n",
        "An index has several data structures:\n",
        "\n",
        "*    **the CollectionStatistics**- the salient global statistics of the index.\n",
        "*    **the Lexicon** - the vocabulary of the index, including statistics of the terms, and a pointer into the inverted index.\n",
        "\n",
        "* **the inverted index (a PostingIndex**) - contains the posting list for each term, detailing the frequency in which aterm appears in that document .\n",
        "* **the DocumentIndex** - contains the length of the document (and other field lengths).  \n",
        "* **the MetaIndex** - contains document metadata, such as the docno, and optionally the raw text and the URL ofeach document.\n",
        "* **the direct index (also a PostingIndex)** - contains a posting list for each document, detailing which terms occuringthat document and which frequency. The presence of the direct index depends on the IndexingType that has beenapplied - single-pass and some memory indices do not provide a direct index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-LtoA8vymt_"
      },
      "source": [
        "Let's check the files the index files created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxi7QzSFZVm4"
      },
      "source": [
        "!ls -lh myFirstIndex/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTLrusMevF5A"
      },
      "source": [
        "We can export our index into our machine as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1M-hcpyvMOo"
      },
      "source": [
        "# from google.colab import files\n",
        "# !zip -r ./myFirstIndex.zip ./myFirstIndex\n",
        "# files.download(\"myFirstIndex.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NcUt2Rxg6gW"
      },
      "source": [
        "Let's check the statistics about the index we created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxVw5bIYZ5uw"
      },
      "source": [
        "print(index_ref.toString())\n",
        "#we will first load the index\n",
        "index = pt.IndexFactory.of(index_ref)\n",
        "#we will call getCollectionStatistics() to check the stats\n",
        "print(index.getCollectionStatistics().toString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBzDWtM2atGK"
      },
      "source": [
        "We can check the lexicon which is the **vocabulary** of the collection.\n",
        "\n",
        "* Nt is the number of unique documents that each term occurs in.\n",
        "* TF is the total number of occurrences – some weighting models use this instead of Nt.\n",
        "* The numbers in the @{} are a pointer – they tell Terrier where the postings are for that term in the inverted index data structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPWT1ZtawgJ"
      },
      "source": [
        "for kv in index.getLexicon():\n",
        "  print(\"%s -> %s \" % (kv.getKey(), kv.getValue().toString())) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiMOrOPCs4Sv"
      },
      "source": [
        "we can also lookup a term in PyTerrier's lexicon:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fioTorgKs-Y9"
      },
      "source": [
        "index.getLexicon()[\"عرب\"].toString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtrXOoICfYR5"
      },
      "source": [
        "**The inverted index** tells us in which documents each term occurs in. \n",
        "The LexiconEntry is the pointer that tell us where to find the postings for that term in the inverted index.\n",
        "\n",
        "Let's look in which documents the word \"العرب\" occurs and its frequency in each document.\n",
        "\n",
        "**Note:** we need to preprocess each search term with the same preprocessing steps we performed on the collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_DD4WJEfAoW"
      },
      "source": [
        "#preprocess the search term\n",
        "term=\"العرب\"\n",
        "print(\"the term before normalization and stemming:\", term)\n",
        "#normalize the word\n",
        "term=normalize(term)\n",
        "#stem the word\n",
        "term=ar_stemmer.stemWord(term)\n",
        "print(\"the term after normalization and stemming:\", term)\n",
        "#search the term\n",
        "try:\n",
        " pointer = index.getLexicon()[term]\n",
        " for posting in index.getInvertedIndex().getPostings(pointer):\n",
        "    print(posting.toString() + \" doclen=%d\" % posting.getDocumentLength())\n",
        "except:\n",
        "    print(\"term %s not found\"%term)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQWNiJUKf-7r"
      },
      "source": [
        "How many documents does term \"العرب\" occur in?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-cAflIvgRr3"
      },
      "source": [
        "index.getLexicon()[term].getDocumentFrequency()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gI042-6hQC_"
      },
      "source": [
        "What terms occur in the 4th document?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeMp2e9chTYm"
      },
      "source": [
        "di = index.getDirectIndex()\n",
        "doi = index.getDocumentIndex()\n",
        "lex = index.getLexicon()\n",
        "docid = 3 #docids are 0-based #note: postings will be null if the document is empty\n",
        "for posting in di.getPostings(doi.getDocumentEntry(docid)):\n",
        "    termid = posting.getId()\n",
        "    lee = lex.getLexiconEntry(termid)\n",
        "    print(\"%s with frequency %d\" % (lee.getKey(),posting.getFrequency()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1iElbcYpV9o"
      },
      "source": [
        "### **Indexing a bigger collection**\n",
        "**[EveTAR](https://link.springer.com/article/10.1007/s10791-017-9325-7)**, the first freely-available tweet test collection for multiple IR tasks. EveTAR includes a crawl of 355M Arabic tweets and covers 50 significant events for which about 62K tweets were judged with substantial average inter-annotator agreement (Kappa value of 0.71).\n",
        "\n",
        "First, we need to read the data from our Github repository. Note that we will use only a subset of 50K tweets in this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkRy1YCTS4V9"
      },
      "source": [
        "dataset_links=[\"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-01.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-02.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-03.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-04.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-05.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-06.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-07.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-08.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-09.txt\",\n",
        "               \"https://raw.githubusercontent.com/telsayed/IR-in-Arabic/master/Summer2021/data/EveTAR/tweets/evetar-q-10.txt\"]\n",
        "\n",
        "full_data=pd.DataFrame()\n",
        "for i in tqdm(range(len(dataset_links))):\n",
        "    tweets=pd.read_csv(dataset_links[i], sep='\\t')\n",
        "    full_data=pd.concat([full_data,tweets],ignore_index=True)\n",
        "full_data.reset_index(inplace=True,drop=True)\n",
        "full_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5I2tWowd053"
      },
      "source": [
        "\n",
        "#the docno will be our tweetID\n",
        "full_data[\"docno\"]=full_data[\"tweetID\"].astype(str)\n",
        "full_data[[\"docno\"]]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InAqXt9hUwD-"
      },
      "source": [
        "We will perform the same processing steps mentioned above but because we will index a collection of the tweets we need to clean the tweets before the other processsing steps them (remove the urls, emojies....)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olgGJPcXV1yA"
      },
      "source": [
        "#a function to clean the tweets\n",
        "def clean(text):\n",
        "   text = re.sub(r\"http\\S+\", \" \", text) # remove urls\n",
        "   text = re.sub(r\"RT \", \" \", text) # remove rt\n",
        "   text = re.sub(r\"@[\\w]*\", \" \", text) # remove handles\n",
        "   text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text) # remove special characters\n",
        "   text = re.sub(r'\\t', ' ', text) # remove tabs\n",
        "   text = re.sub(r'\\n', ' ', text) # remove line jump\n",
        "   text = re.sub(r\"\\s+\", \" \", text) # remove extra white space\n",
        "   accents = re.compile(r'[\\u064b-\\u0652\\u0640]') # harakaat and tatweel (kashida) to remove\n",
        "     \n",
        "   arabic_punc= re.compile(r'[\\u0621-\\u063A\\u0641-\\u064A\\d+]+') # Keep only Arabic letters/do not remove number\n",
        "   text=' '.join(arabic_punc.findall(accents.sub('',text)))\n",
        "   text = text.strip()\n",
        "   return text\n",
        "\n",
        "\n",
        "#we will clean each tweet in the collection\n",
        "full_data[\"text\"]=full_data[\"tweetText\"].apply(clean)\n",
        "print(\"***************************************************************************Tweets after cleaning*********************************************************************\")\n",
        "full_data[['docno','tweetText','text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgeqGPsXbgJU"
      },
      "source": [
        "We will remove the stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKHnIHzXblKx"
      },
      "source": [
        "full_data[\"text\"]=full_data[\"text\"].apply(remove_stopWords)\n",
        "print(\"***************************************************************************Tweets after removing stopWords*********************************************************************\")\n",
        "full_data[['docno','tweetText','text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDVI_tfmV5Bv"
      },
      "source": [
        "We also need to normalize the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7aXdEvVV9OA"
      },
      "source": [
        "#we will normalize using our normalize function. \n",
        "full_data[\"text\"]=full_data[\"text\"].apply(normalize)\n",
        "print(\"***************************************************************************Tweets after normalizing*********************************************************************\")\n",
        "full_data[['docno','tweetText','text']]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1ImdTGoV8cL"
      },
      "source": [
        "Stemming the collection (this will take up 2 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ompjQWlk418G"
      },
      "source": [
        "full_data['text']=full_data['text'].apply(stem)\n",
        "print(\"***************************************************************************Tweets after stemming*********************************************************************\")\n",
        "full_data[['docno','tweetText','text']]   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc8QvYbgB5xh"
      },
      "source": [
        "### **Indexing EveTAR**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niRimiCZiO-k"
      },
      "source": [
        "indexer = pt.DFIndexer(\"./evetarIndex\", overwrite=True)\n",
        "#as the default id an English tokenizer we will update it by setting it to a non-English tokenizer \"UTFTokenizer\"\n",
        "indexer.setProperty(\"tokeniser\", \"UTFTokeniser\")\n",
        "index_ref = indexer.index(full_data[\"text\"], full_data[\"docno\"])\n",
        "index_ref.toString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM_xXbhKuktQ"
      },
      "source": [
        "### **Explore the index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9hXOPp0Sob"
      },
      "source": [
        "#we will first load the index\n",
        "index = pt.IndexFactory.of(index_ref)\n",
        "#we will call getCollectionStatistics() to check the stats\n",
        "print(index.getCollectionStatistics().toString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCcubOtayRO"
      },
      "source": [
        "Let's check the vocab in our index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuCQENo1vFFf"
      },
      "source": [
        "#check the vocab\n",
        "for kv in index.getLexicon():\n",
        "  print(\"%s -> %s \" % (kv.getKey(), kv.getValue().toString())) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FVdZjX5vHMg"
      },
      "source": [
        "### **Exercise1**\n",
        "How many documents mention your country name? which documents are those?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT2ORK4HwLBj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DElk2mxEvOoE"
      },
      "source": [
        "### **Exercise2**\n",
        "Select any document from the collection and check which of its terms appear in the index?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lwNWMCNwKaM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAhILb3S_ChS"
      },
      "source": [
        "### **Exercise3**\n",
        "How can we update our index to include the positions of the terms in the index? Hint: you can use [PyTerrier documentation](https://pyterrier.readthedocs.io/_/downloads/en/latest/pdf/) as a reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baW1NO1l--hC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8ZUAJKhwL0J"
      },
      "source": [
        "### **Exercise4**\n",
        "Index an Arabic collection of your choice. You can use the Arabic datasets available at [Huggingface](https://huggingface.co/datasets?filter=languages:ar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrftyM9kwFc9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_fMCB2FXIG"
      },
      "source": [
        "### **References**\n",
        "\n",
        "\n",
        "* [Pandas DataFrames documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).  \n",
        "* IR From Bag-of-words to BERT and Beyond through Practical Experiments. [PyTerrier ECIR2021 Tutorial](https://github.com/terrier-org/ecir2021tutorial).\n",
        "*   [PyTerrier documentation.](https://pyterrier.readthedocs.io/_/downloads/en/latest/pdf/)\n",
        "* [Processing Arabic text in Python](https://alraqmiyyat.github.io/2013/01-02.html).\n",
        "\n"
      ]
    }
  ]
}